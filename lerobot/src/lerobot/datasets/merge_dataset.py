#!/usr/bin/env python
"""
Dataset merging utility for LeRobot datasets.

This script merges multiple moving-tape datasets into a single cookingbot dataset.
Fixed issues:
- Fixed undefined variable 'datasets_dir' 
- Improved episode indexing logic
- Enhanced frame processing with better error handling
- Improved image dimension conversion for different tensor types
- Added progress tracking and better logging
- Made the merge process more robust with graceful error handling
"""

import os
import json
from lerobot.datasets.lerobot_dataset import LeRobotDataset, MultiLeRobotDataset

def find_available_datasets():
    """í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ moving-tape ë°ì´í„°ì…‹ë“¤ì„ ì°¾ìŒ"""
    
    # ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ê²½ë¡œë“¤
    possible_paths = [
        ".",  # í˜„ì¬ ë””ë ‰í† ë¦¬
        "~/lerobotjs/datasets",  # ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ë¡œ
        os.path.expanduser("~/lerobotjs/datasets"),  # í™•ì¥ëœ ê²½ë¡œ
        "/home/son/lerobotjs/datasets",  # ì ˆëŒ€ ê²½ë¡œ
    ]
    
    datasets_path = None
    datasets = []
    
    # ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ë¡œ ì°¾ê¸°
    for path in possible_paths:
        expanded_path = os.path.expanduser(path)
        if os.path.exists(expanded_path):
            print(f"ê²½ë¡œ í™•ì¸ ì¤‘: {expanded_path}")
            temp_datasets = []
            try:
                for item in os.listdir(expanded_path):
                    item_path = os.path.join(expanded_path, item)
                    if item.startswith('moving-tape-dataset') and os.path.isdir(item_path):
                        info_path = os.path.join(item_path, 'meta', 'info.json')
                        if os.path.exists(info_path):
                            try:
                                with open(info_path, 'r') as f:
                                    info = json.load(f)
                                temp_datasets.append((item, item_path))
                                print(f"âœ“ {item} - ì ‘ê·¼ ê°€ëŠ¥ (ì—í”¼ì†Œë“œ: {info.get('total_episodes', '?')}ê°œ)")
                            except Exception as e:
                                print(f"âœ— {item} - ì ‘ê·¼ ë¶ˆê°€: {e}")
                
                if temp_datasets:
                    datasets_path = expanded_path
                    datasets = temp_datasets
                    break
                    
            except Exception as e:
                print(f"âœ— {expanded_path} ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                continue
    
    if not datasets:
        print("moving-tape ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤:")
        for path in possible_paths:
            expanded_path = os.path.expanduser(path)
            print(f"  - {expanded_path} ({'ì¡´ì¬' if os.path.exists(expanded_path) else 'ì—†ìŒ'})")
        return [], None
    
    print(f"\në°ì´í„°ì…‹ ë°œê²¬ ê²½ë¡œ: {datasets_path}")
    return [item[0] for item in datasets], datasets_path

def upload_datasets_first(dataset_paths, datasets_root):
    """ë°ì´í„°ì…‹ë“¤ì„ ë¨¼ì € í—ˆê¹…í˜ì´ìŠ¤ì— ê°œë³„ ì—…ë¡œë“œ"""
    repo_ids = []
    
    print("ë°ì´í„°ì…‹ë“¤ì„ í—ˆê¹…í˜ì´ìŠ¤ì— ê°œë³„ ì—…ë¡œë“œ ì¤‘...")
    print("=" * 50)
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ ì €ì¥
    original_dir = os.getcwd()
    
    try:
        # ë°ì´í„°ì…‹ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ì´ë™
        os.chdir(datasets_root)
        print(f"ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½: {datasets_root}")
        
        for i, dataset_path in enumerate(dataset_paths):
            repo_id = f"JisooSong/{dataset_path}"
            
            print(f"\n[{i+1}/{len(dataset_paths)}] {dataset_path} ì—…ë¡œë“œ ì¤‘...")
            
            try:
                # ë¡œì»¬ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì—…ë¡œë“œ
                dataset = LeRobotDataset(repo_id, root='.')
                dataset.push_to_hub(
                    tags=["robotics", "manipulation", "moving-tape"],
                    private=False,
                    push_videos=True
                )
                
                repo_ids.append(repo_id)
                print(f"âœ“ ì—…ë¡œë“œ ì™„ë£Œ: {repo_id}")
                
            except Exception as e:
                print(f"âœ— ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•´ë„ repo_idëŠ” ì¶”ê°€ (ì´ë¯¸ ì—…ë¡œë“œë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
                repo_ids.append(repo_id)
    
    finally:
        # ì›ë˜ ë””ë ‰í† ë¦¬ë¡œ ë³µì›
        os.chdir(original_dir)
    
    return repo_ids

def merge_with_multi_dataset(repo_ids, merged_name="moving-tape-merged-dataset"):
    """MultiLeRobotDatasetì„ ì‚¬ìš©í•´ì„œ ë°ì´í„°ì…‹ë“¤ì„ í•©ì¹˜ê¸°"""
    
    print(f"\nMultiLeRobotDatasetì„ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ í†µí•©")
    print("=" * 50)
    
    # ì„ì‹œ ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„± (ì‹œìŠ¤í…œ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
    import tempfile
    import shutil
    
    temp_dir = tempfile.mkdtemp(prefix="lerobot_merge_")
    print(f"ì„ì‹œ ì‘ì—… ë””ë ‰í† ë¦¬: {temp_dir}")
    
    try:
        # 1. MultiLeRobotDataset ìƒì„± (ì„ì‹œ ë””ë ‰í† ë¦¬ì—)
        print(f"MultiLeRobotDataset ìƒì„± ì¤‘...")
        print(f"ëŒ€ìƒ repo_ids: {repo_ids}")
        
        try:
            multi_dataset = MultiLeRobotDataset(
                repo_ids=repo_ids,
                root=temp_dir,  # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                video_backend="pyav"  # torchcodec ëŒ€ì‹  pyav ì‚¬ìš©
            )
        except Exception as e:
            print(f"âœ— MultiLeRobotDataset ìƒì„± ì‹¤íŒ¨: {e}")
            print("ëŒ€ì•ˆìœ¼ë¡œ ê°œë³„ ë°ì´í„°ì…‹ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            # ê°œë³„ ë°ì´í„°ì…‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ëŒ€ì•ˆ ë¡œì§ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŒ
            raise e
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜
        import os
        os.environ['HF_HUB_TIMEOUT'] = '300'  # 5ë¶„ìœ¼ë¡œ ì„¤ì •
        
        print(f"âœ“ MultiLeRobotDataset ìƒì„± ì™„ë£Œ")
        print(f"  ì´ ì—í”¼ì†Œë“œ: {multi_dataset.num_episodes}")
        print(f"  ì´ í”„ë ˆì„: {multi_dataset.num_frames}")
        print(f"  FPS: {multi_dataset.fps}")
        print(f"  ì¹´ë©”ë¼ í‚¤: {multi_dataset.camera_keys}")
        
        # 2. ê¸°ì¡´ cookingbot ì €ì¥ì†Œë¥¼ ë¡œë“œí•´ì„œ ë³‘í•© ë°ì´í„° ì¶”ê°€
        print(f"\nê¸°ì¡´ cookingbot ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        
        # datasets_dir ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •ì˜
        datasets_dir = os.path.expanduser("~/lerobotjs/datasets")
        
        # í•­ìƒ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ìƒì„± (ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©í•˜ì§€ ì•ŠìŒ)
        print(f"ìƒˆë¡œìš´ í†µí•© ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ë°ì´í„°ì…‹ì´ ìˆë‹¤ë©´ ì œê±°
        merged_path = os.path.join(datasets_dir, "JisooSong-cookingbot")
        if os.path.exists(merged_path):
            print(f"ê¸°ì¡´ ë°ì´í„°ì…‹ ì œê±° ì¤‘: {merged_path}")
            shutil.rmtree(merged_path)
        
        # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì˜ ì •ë³´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒˆ ë°ì´í„°ì…‹ ìƒì„±
        first_dataset = multi_dataset._datasets[0]
        
        # LeRobotDataset.createì„ ì‚¬ìš©í•´ì„œ ìƒˆ ë°ì´í„°ì…‹ ìƒì„±
        merged_dataset = LeRobotDataset.create(
            repo_id="JisooSong/cookingbot",
            fps=multi_dataset.fps,
            features=first_dataset.features,
            root=datasets_dir,
            robot_type=first_dataset.meta.robot_type,
            use_videos=len(first_dataset.meta.video_keys) > 0
        )
        
        print(f"âœ“ í†µí•© ë°ì´í„°ì…‹ ìƒì„±: {merged_dataset.repo_id}")
        
        # 3. MultiLeRobotDatasetì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ìƒˆ ë°ì´í„°ì…‹ì— ë³µì‚¬
        print(f"\në°ì´í„° ë³µì‚¬ ì‹œì‘...")
        
        total_episodes = sum(ds.num_episodes for ds in multi_dataset._datasets)
        processed_episodes = 0
        successful_episodes = 0
        failed_episodes = 0
        
        # ê° ì›ë³¸ ë°ì´í„°ì…‹ë³„ë¡œ ì—í”¼ì†Œë“œ ì²˜ë¦¬
        for dataset_idx, original_dataset in enumerate(multi_dataset._datasets):
            dataset_name = repo_ids[dataset_idx]
            print(f"\n[{dataset_idx + 1}/{len(multi_dataset._datasets)}] {dataset_name} ì²˜ë¦¬ ì¤‘...")
            print(f"  ì´ ì—í”¼ì†Œë“œ: {original_dataset.num_episodes}ê°œ")
            
            # í•´ë‹¹ ë°ì´í„°ì…‹ì˜ ì—í”¼ì†Œë“œë“¤ ì²˜ë¦¬
            for local_ep_idx in range(original_dataset.num_episodes):
                processed_episodes += 1
                print(f"  ì—í”¼ì†Œë“œ {local_ep_idx + 1}/{original_dataset.num_episodes} ì²˜ë¦¬ ì¤‘... (ì „ì²´ ì§„í–‰ë¥ : {processed_episodes}/{total_episodes})")
                
                # ì—í”¼ì†Œë“œì˜ ì‹œì‘ê³¼ ë í”„ë ˆì„ ì°¾ê¸°
                ep_start = original_dataset.episode_data_index["from"][local_ep_idx].item()
                ep_end = original_dataset.episode_data_index["to"][local_ep_idx].item()
                
                print(f"    í”„ë ˆì„ ë²”ìœ„: {ep_start} ~ {ep_end-1} (ì´ {ep_end - ep_start}ê°œ)")
                
                # ì—í”¼ì†Œë“œ ë²„í¼ ì´ˆê¸°í™” (ìƒˆ ì—í”¼ì†Œë“œ ì‹œì‘)
                merged_dataset.episode_buffer = merged_dataset.create_episode_buffer(episode_index=merged_dataset.num_episodes)
                
                # ì—í”¼ì†Œë“œì˜ ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬
                frame_count = 0
                for frame_idx in range(ep_start, ep_end):
                    try:
                        # ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ì§ì ‘ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì „í•¨)
                        frame_data = original_dataset[frame_idx]
                        
                        # task ì •ë³´ ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ì—ì„œ ì°¾ê¸°)
                        task = (frame_data.get('task') or 
                               frame_data.get('task_name') or 
                               'default_task')
                        
                        # ìƒˆ ë°ì´í„°ì…‹ì— ì¶”ê°€í•  í”„ë ˆì„ ë°ì´í„° ì¤€ë¹„
                        new_frame_data = {}
                        for key, value in frame_data.items():
                            # ë©”íƒ€ë°ì´í„° í‚¤ë“¤ì€ ì œì™¸
                            if key in ['episode_index', 'index', 'frame_index', 'timestamp', 
                                     'task_index', 'task', 'task_name', 'dataset_index']:
                                continue
                            
                            # ì´ë¯¸ì§€ ë°ì´í„°ì¸ ê²½ìš° ì°¨ì› ë³€í™˜ (CHW -> HWC)
                            if 'images' in key and hasattr(value, 'shape') and len(value.shape) == 3:
                                if value.shape[0] == 3:  # (3, H, W) í˜•ì‹ì¸ ê²½ìš°
                                    # (3, H, W) -> (H, W, 3)ë¡œ ë³€í™˜
                                    import torch
                                    import numpy as np
                                    try:
                                        if isinstance(value, torch.Tensor):
                                            value = value.permute(1, 2, 0)  # pytorch tensor
                                        elif isinstance(value, np.ndarray):
                                            value = value.transpose(1, 2, 0)  # numpy array
                                        else:
                                            # ë‹¤ë¥¸ íƒ€ì…ì˜ ê²½ìš° numpyë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
                                            value = np.array(value)
                                            if value.shape[0] == 3:
                                                value = value.transpose(1, 2, 0)
                                        print(f"      ì´ë¯¸ì§€ ì°¨ì› ë³€í™˜: {key} {value.shape}")
                                    except Exception as img_error:
                                        print(f"      âš  ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ {key}: {img_error}, ì›ë³¸ ìœ ì§€")
                                        # ë³€í™˜ ì‹¤íŒ¨ì‹œ ì›ë³¸ ìœ ì§€
                                        pass
                            
                            new_frame_data[key] = value
                        
                        # í”„ë ˆì„ ì¶”ê°€
                        merged_dataset.add_frame(new_frame_data, task)
                        frame_count += 1
                        
                    except Exception as frame_error:
                        print(f"      âœ— í”„ë ˆì„ {frame_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {frame_error}")
                        print(f"      í”„ë ˆì„ ë°ì´í„° í‚¤: {list(frame_data.keys()) if 'frame_data' in locals() else 'ì—†ìŒ'}")
                        # ê°œë³„ í”„ë ˆì„ ì‹¤íŒ¨ì‹œì—ë„ ê³„ì† ì§„í–‰
                        print(f"      âš  í”„ë ˆì„ {frame_idx} ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰...")
                        continue
                
                print(f"    ì—í”¼ì†Œë“œ ì™„ë£Œ: {frame_count}ê°œ í”„ë ˆì„ ì²˜ë¦¬ë¨")
                
                # ê° ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œë§ˆë‹¤ ì €ì¥
                try:
                    merged_dataset.save_episode()
                    print(f"    âœ“ ì—í”¼ì†Œë“œ {local_ep_idx + 1} ì €ì¥ ì™„ë£Œ")
                    successful_episodes += 1
                except Exception as save_error:
                    print(f"    âœ— ì—í”¼ì†Œë“œ {local_ep_idx + 1} ì €ì¥ ì‹¤íŒ¨: {save_error}")
                    failed_episodes += 1
                    # ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                    continue
        
        print(f"\nâœ“ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ!")
        print(f"  ì„±ê³µí•œ ì—í”¼ì†Œë“œ: {successful_episodes}ê°œ")
        print(f"  ì‹¤íŒ¨í•œ ì—í”¼ì†Œë“œ: {failed_episodes}ê°œ")
        print(f"  ì´ ì—í”¼ì†Œë“œ: {merged_dataset.num_episodes}")
        print(f"  ì´ í”„ë ˆì„: {merged_dataset.num_frames}")
        
        # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì¡°ê¸° ì¢…ë£Œ
        if successful_episodes == 0:
            print(f"\nâŒ ì„±ê³µí•œ ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. í†µí•©ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return None
        
        # ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬
        print(f"\në°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ ì¤‘...")
        try:
            # ê¸°ë³¸ í†µê³„ í™•ì¸
            print(f"  ì—í”¼ì†Œë“œ ìˆ˜: {merged_dataset.num_episodes}")
            print(f"  í”„ë ˆì„ ìˆ˜: {merged_dataset.num_frames}")
            print(f"  FPS: {merged_dataset.fps}")
            print(f"  ì¹´ë©”ë¼ í‚¤: {merged_dataset.camera_keys}")
            print(f"  ë¹„ë””ì˜¤ í‚¤: {merged_dataset.video_keys}")
            
            # ì²« ë²ˆì§¸ í”„ë ˆì„ìœ¼ë¡œ ë°ì´í„° êµ¬ì¡° í™•ì¸
            if merged_dataset.num_frames > 0:
                sample_frame = merged_dataset[0]
                print(f"  ìƒ˜í”Œ í”„ë ˆì„ í‚¤: {list(sample_frame.keys())}")
                print(f"  ìƒ˜í”Œ í”„ë ˆì„ íƒ€ì…: {[(k, type(v)) for k, v in sample_frame.items()]}")
            
            print(f"âœ“ ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼")
            
        except Exception as validation_error:
            print(f"âœ— ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {validation_error}")
            print(f"ì—…ë¡œë“œë¥¼ ê±´ë„ˆë›°ê³  ë¡œì»¬ ë°ì´í„°ì…‹ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return merged_dataset
        
        # 4. cookingbot ì €ì¥ì†Œì— ë³‘í•© ë°ì´í„° ì—…ë¡œë“œ
        print(f"\ncookingbot ì €ì¥ì†Œì— ë³‘í•© ë°ì´í„° ì—…ë¡œë“œ ì¤‘...")
        try:
            # ì—…ë¡œë“œ ì „ ìµœì¢… í™•ì¸
            if merged_dataset.num_frames == 0:
                print(f"âš  ê²½ê³ : ë°ì´í„°ì…‹ì— í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return merged_dataset
            
            print(f"ì—…ë¡œë“œ ì‹œì‘ - ì—í”¼ì†Œë“œ: {merged_dataset.num_episodes}, í”„ë ˆì„: {merged_dataset.num_frames}")
            
            # ë°ì´í„°ì…‹ì´ ì œëŒ€ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
            print(f"ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ: {merged_dataset.root}")
            print(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ ì¡´ì¬ í™•ì¸:")
            meta_files = ["info.json", "episodes.jsonl", "tasks.jsonl", "stats.json"]
            for meta_file in meta_files:
                meta_path = merged_dataset.root / "meta" / meta_file
                exists = meta_path.exists()
                print(f"  {meta_file}: {'âœ“' if exists else 'âœ—'}")
            
            # ë°ì´í„° íŒŒì¼ í™•ì¸
            data_dir = merged_dataset.root / "data"
            if data_dir.exists():
                chunk_dirs = [d for d in data_dir.iterdir() if d.is_dir() and d.name.startswith("chunk-")]
                print(f"  ë°ì´í„° ì²­í¬: {len(chunk_dirs)}ê°œ")
                for chunk_dir in chunk_dirs:
                    parquet_files = list(chunk_dir.glob("*.parquet"))
                    print(f"    {chunk_dir.name}: {len(parquet_files)}ê°œ parquet íŒŒì¼")
            
            merged_dataset.push_to_hub(
                tags=["robotics", "manipulation", "merged-dataset", "moving-tape"],
                private=False,
                push_videos=True
            )
            
            print(f"âœ“ cookingbot ì €ì¥ì†Œ ì—…ë¡œë“œ ì™„ë£Œ!")
            print(f"URL: https://huggingface.co/datasets/JisooSong/cookingbot")
            
        except Exception as upload_error:
            print(f"âœ— ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_error}")
            print(f"ì—ëŸ¬ íƒ€ì…: {type(upload_error).__name__}")
            print(f"ì—ëŸ¬ ìƒì„¸: {str(upload_error)}")
            
            # ì—…ë¡œë“œ ì‹¤íŒ¨í•´ë„ ë¡œì»¬ ë°ì´í„°ì…‹ì€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë¨
            print(f"âš  ë¡œì»¬ ë°ì´í„°ì…‹ì€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ë¡œì»¬ ê²½ë¡œ: {merged_dataset.root}")
            print(f"ìˆ˜ë™ìœ¼ë¡œ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            
            return merged_dataset
        
        return merged_dataset
        
    except Exception as e:
        print(f"âœ— í†µí•© ì‹¤íŒ¨: {e}")
        print(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        print(f"ì—ëŸ¬ ìƒì„¸: {str(e)}")
        import traceback
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
        traceback.print_exc()
        return None
    
    finally:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        print(f"\nì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘: {temp_dir}")
        try:
            shutil.rmtree(temp_dir)
            print(f"âœ“ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš  ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            print(f"ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œí•´ì£¼ì„¸ìš”: {temp_dir}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("MultiLeRobotDatasetì„ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ í†µí•©")
    print("=" * 60)
    
    # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ì°¾ê¸°
    available_datasets, datasets_root = find_available_datasets()
    
    if len(available_datasets) < 1:
        print("í†µí•©í•  ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹: {len(available_datasets)}ê°œ")
    
    # 2. ë°ì´í„°ì…‹ ì„ íƒ
    print(f"\nì–´ë–¤ ë°ì´í„°ì…‹ë“¤ì„ í†µí•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("1. ì²˜ìŒ 2ê°œ ë°ì´í„°ì…‹ë§Œ")
    print("2. ëª¨ë“  ë°ì´í„°ì…‹")
    print("3. ìˆ˜ë™ìœ¼ë¡œ ì„ íƒ (y/n ë°©ì‹)")
    print("4. ì§ì ‘ ì…ë ¥")
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1/2/3/4): ").strip()
    
    if choice == "1":
        datasets_to_merge = available_datasets[:2]
    elif choice == "2":
        datasets_to_merge = available_datasets
    elif choice == "3":
        datasets_to_merge = []
        print("\ní†µí•©í•  ë°ì´í„°ì…‹ë“¤ì„ ì„ íƒí•˜ì„¸ìš”:")
        for i, dataset in enumerate(available_datasets):
            while True:
                answer = input(f"{dataset}ì„ í¬í•¨í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                if answer in ['y', 'yes']:
                    datasets_to_merge.append(dataset)
                    break
                elif answer in ['n', 'no']:
                    break
                else:
                    print("y ë˜ëŠ” nìœ¼ë¡œ ë‹µí•´ì£¼ì„¸ìš”.")
    elif choice == "4":
        datasets_to_merge = []
        print("\ní†µí•©í•  ë°ì´í„°ì…‹ ì´ë¦„ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„):")
        print("ì˜ˆ: moving-tape-dataset-20250915_153200, moving-tape-dataset-20250915_154309")
        
        while True:
            user_input = input("ë°ì´í„°ì…‹ ì´ë¦„ë“¤: ").strip()
            if not user_input:
                print("ìµœì†Œ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ì€ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
                continue
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°
            input_datasets = [name.strip() for name in user_input.split(',')]
            
            # ì…ë ¥ëœ ë°ì´í„°ì…‹ë“¤ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            valid_datasets = []
            invalid_datasets = []
            
            for dataset in input_datasets:
                if dataset in available_datasets:
                    valid_datasets.append(dataset)
                else:
                    invalid_datasets.append(dataset)
            
            if invalid_datasets:
                print(f"\në‹¤ìŒ ë°ì´í„°ì…‹ë“¤ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {invalid_datasets}")
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ë“¤: {available_datasets}")
                continue
            
            datasets_to_merge = valid_datasets
            break
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ì²˜ìŒ 2ê°œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        datasets_to_merge = available_datasets[:2]
    
    if len(datasets_to_merge) < 1:
        print("ì„ íƒëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\ní†µí•© ëŒ€ìƒ: {datasets_to_merge}")
    
    # 3. ë¨¼ì € ê°œë³„ ë°ì´í„°ì…‹ë“¤ì„ í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œ
    repo_ids = upload_datasets_first(datasets_to_merge, datasets_root)
    
    # 4. MultiLeRobotDatasetìœ¼ë¡œ í†µí•©
    merged_dataset = merge_with_multi_dataset(repo_ids)
    
    if merged_dataset:
        print(f"\nğŸ‰ í†µí•© ì™„ë£Œ!")            